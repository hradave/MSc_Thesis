%%% lorem.tex --- 
%% 
%% Filename: lorem.tex
%% Description: 
%% Author: Ola Leifler
%% Maintainer: 
%% Created: Wed Nov 10 09:59:23 2010 (CET)
%% Version: $Id$
%% Version: 
%% Last-Updated: Tue Oct  4 11:58:17 2016 (+0200)
%%           By: Ola Leifler
%%     Update #: 7
%% URL: 
%% Keywords: 
%% Compatibility: 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Commentary: 
%% 
%% 
%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Change log:
%% 
%% 
%% RCS $Log$
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% 
%%% Code:

\chapter{Related work}
\label{cha:related_work}

Numerous researchers have applied computer algorithms for histology image analysis, and they can be split into two groups. One requires the extraction of hand-crafted features that expert pathologists would recognize from the slide images, the other does not. The second group can afford to not explicitly obtain these features, because they use deep convolutional networks to automatically do that within the model. The first group therefore uses more traditional machine learning algorithms that accept well defined features. This thesis belongs in the second group, and the author aims to achieve higher accuracy scores with deep learning than the researchers did in the first group.

This is not an easy task, because some researchers achieved very good results without using deep learning. \cite{BARKER201660} extracted coarse features of shape, color and texture from the image patches focusing on cell nuclei, reduced the dimensionality of the features, and created clusters representing similar patterns. Fine features were extracted from a select few patches from each cluster, then an Elastic Net was used on these patches to make the diagnostic decision of classifying a slide into GBM or LGG. Extracting fine features was a very computationally expensive task, this is why only a smaller number of representative patches were a part of this step. Slide level aggregation was done by weighted voting of the predictions of the patches. All Whole Slide Images came from the TCGA data repository, and were resized to 20x magnification level using bicubic interpolation, and the patches were 1024 x 1024 pixels. They achieved an accuracy of 93.1\% on a dataset containing 302 brain cancer cases.

\cite{cancers12030578} approached the problem similarly, by using a more traditional machine learning model, the support vector machine. They extracted features from the texture, intensity and morphology along with several clinical measures, and trained the SVM model on them. The creation of features required knowledge about what differentiates GBM from LGG, such as microvascular proliferation, mitotic activity and necrosis. The validation accuracy was 75.12\% on images obtained from TCGA.

There is a lot more literature in this field that utilized the power of deep learning for image analysis. \cite{Kurc2020} presented the three best performing methods from the 21st International Medical Image Computing and Computer Assisted Intervention (MICCAI 2018) conference for classification of oligodendroglioma and astrocytoma (which are two subclasses of LGG) patients. They all used a combination of radiographic and histologic image dataset, where the histologic images were obtained from TCGA, but they processed the two types of images separately. The three methods achieved accuracy scores of 90\%, 80\% and 75\% respectively. 

The best one, \cite{bagari2019} applied several preprocessing steps on the images, including region of interest detection, stain normalization and patch extraction (224 x 224). They trained an autoencoder with convolutional layers to extract features from each patch, then used these features to identify patches that can potentially contain tumor regions using anomaly detection (Isolation Forest), where tumor was considered the anomaly. The DenseNet-161 network, pretrained on ImageNet, was then trained on these anomaly patches only, and the final prediction was done according to majority voting. 

The second best approach, \cite{momeni2018} argues that since only whole slide level annotation is available, but the training is done on patches, this is a weakly-supervised learning problem. To tackle this, they incorporated a Multiple Instance Learning (MIL) framework into the CNN architecture, which helps combine the patch predictions to slide level intelligently. The preprocessing steps were similar to \cite{bagari2019}, but they used 256 x 256 patches and a more simple histogram equalization for color normalization. Here a pretrained DenseNet-169 model was used with dropout regularization, which produced an average slide level score from all sampled patches for that slide. They concluded that the dropout technique did not improve the accuracy significantly. 

The third best solution (only described in \cite{Kurc2020}), used a different approach. They identified tissue characteristics that differentiate the two classes, such as necrosis, cell density, cell shape and blood vessels. The images were then partitioned into 512 x 512 patches, and fed into a VGG16 CNN network with data augmentation to tackle class imbalance, and dropout and batch normalization layers to reduce overfitting.

A mixture of traditional machine learning and deep learning approaches exists, when the CNNs are only used for automatic feature extraction, but the classification is done by another machine learning algorithm. \cite{xu2017} used a pretrained AlexNet CNN for extracting 4096 features, some of which revealed biological insights, and then employed a SVM. They achieved 97.5\% accuracy and concluded that these CNN features are significantly more powerful than expert-designed features.

\cite{campanella2019} conducted a very extensive research, where they used a deep learning approach to classify whether a slide image has cancer in it or not. They tested their methods on very large datasets of different types of cancer, and different slide preparation methods. The datasets were similar to TCGA in a way that they were also not labeled at pixel level, therefore the authors presented different Multiple Instance Learning approaches to tackle this weakly supervised problem in the form of slide aggregation models. These models included logistic regression, random forest, and recurrent neural networks that were trained on the validation set to avoid overfitting. They showed by statistical comparisons that fully supervised learning models based on curated datasets do not generalize well to real world data, where detailed annotation is not available. Even though the authors did not use brain cancer data, some very useful findings and methods can be applied to the TCGA brain tumor dataset, including the statistical comparison of different models.

Other papers have experimented with deep learning for digital pathology, from which this research can benefit in questions such as optimal patch size, architecture, data augmentation methods, preprocessing steps, and slide level aggregation techniques \cite{janowczyk2016}, \cite{ertosun2015}, \cite{shirazi2020}, \cite{Hamidinekoo2020}, \cite{ker2019}, \cite{wang2019}, \cite{hou2016}.

Deep Learning

Deep learning allows Artificial Neural Networks to learn complex representations of data by discovering structures and patterns automatically. In contrast with traditional machine learning methods, deep learning models do not require domain expertise in designing features, therefore very little engineering is necessary. Deep learning can be used for classification tasks very well, because the network is able to recognize features that are important for discrimination and be robust to irrelevant variations. The learning is done in an iterative way by modifying the weights of the connections between the nodes in the network using the backpropagation algorithm. Each layer learns to recognize features on a higher level of abstraction then the previous one. Images are one type of data that can be analyzed very successfully with deep learning architectures, especially with convolutional neural networks (CNN), because they are much easier to train and generalize much better due to the fact that their adjacent layers are not connected fully \cite{lecun2015}. 

The first paper that used convolutional networks trained by backpropagation for classifying hand-written digits was published in 1990 \cite{lecun1990}, but CNNs became increasingly popular with the arrival of fast graphics processing units (GPUs), and their ability to compute tasks in a massively parallel way making the process of training much faster \cite{raina2009).

Convolutional networks are superior to fully connected networks in image processing, because they are robust to geometric distortions, and the location of features does not matter too much. They also require far fewer images to train thanks to the lower number of connections inside the network \cite{lecun2000}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% lorem.tex ends here

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "demothesis"
%%% End: 
